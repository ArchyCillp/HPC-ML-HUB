---
title: 3. Programming Interface
---

### 3.1. Compilation with NVCC
- kernels -> PTX (汇编) -> cubin
- JIT编译：PTX在运行时被硬件编译成bin，这也是唯一能让应用在编译时没出现的硬件上跑的方法。
- cubin兼容性：8.2可以在8.6上跑，但是不能在7.x或者9.x或者8.0上跑
- PTX兼容性
- 应用兼容性：
	- `-ptx`：生成PTX文件。
	- `-cubin`：生成CUBIN文件。
	- `-gencode`：可以同时生成PTX和CUBIN文件，支持多个架构。
	- `-arch`和`-code`：可以用来指定生成PTX和CUBIN文件的架构。
	- `-arch=sm_70`作为缩写，可以直接生成70的PTX和CUBIN

### 3.2 CUDA runtime
- cudart库，`libcudart.a`，`cudart.so`

### 3.2.1 intialization
- 如果你没有显式调用`cudaSetDevice()`或`cudaInitDevice()`，CUDA运行时会隐式初始化device 0。
- 在测量时间之前，显式调用`cudaSetDevice()`或`cudaInitDevice()`，以确保初始化时间不会被计入后续API调用的时间测量中。
- 在应用程序的早期阶段显式调用`cudaSetDevice()`或`cudaInitDevice()`，并检查返回的错误代码，以确保CUDA运行时的正确初始化。
- primary context
	- `cudaSetDevice()`显式创建
	- 是在第一次调用CUDA runtime函数时隐式创建的
	- host端共享
	- `cudaDeviceReset()`用于销毁当前的primary context
	- 详见CUDA context章节

### 3.2.2 device memory
- 两种类型
	- linear memory（常见）
	- CUDA arrays（Texture & surface memory）
- 拓展memory操作
	- cudaMallocPitch & cudaMalloc3D
		- 用于分配2D和3D数组，保证alignment
		- `cudaMallocPitch(&devPtr,&pitch,width*sizeof(float),height)`
		- devPtr是数组起始地址，pitch是每一行的bytes数量(padding过)，第三个是每一行的需求bytes，height是行的数量
		- pitch(或叫做stride)是一段连续的内存
	- symbol
		- `__device__`或者`__constant__`修饰的变量是device上global memory存放的变量，可以用`cudaMemcpyFromSymbol`, `cudaMemcpyToSymbol` 交互

### 3.2.3 Device Memory L2 Access Management (CC 8.0)
- 数据访问频率
	- persisting: 频繁访问的数据
	- streaming: 只访问一次的数据
- 可以分配一部分(比如75%)L2 cache做set-aside L2 cache，用来自定义存放进L2 cache的数据
- MIG和MPS模式下，该功能不可用或者需要特殊设置
- 由于L2 set-aside是跨kernel共享的，所以分配时要考虑同时执行的所有kernel，L2是否够用
- accessPolicyWindow
	- 可用通过stream或者Graph指定一段global memory被L2 cache加速
	- hitRatio：
		- 如果写0.5，这段内存会有随机选取的50%放入L2 set-aside部分
		- 如果需要cache的区域比L2大，就只出现evict，保留最新访问的部分
		- 因为L2 cache是global的可以跨kernel，因此可以设置低hitRatio防止不同kernel之间相互evict。
	- cudaAccessProperty
		- Streaming：不入L2 cache
		- Persisting：尽量入L2 cache
			- Normal：默认，用于重置；注意如果不重置，kernel结束后可能还是会有内存处于persisting状态，从而影响后续使用，也可以用`cudaCtxResetPersistingL2Cache`来重置。

```C++
cudaStreamAttrValue stream_attribute;                  
stream_attribute.accessPolicyWindow.base_ptr  = reinterpret_cast<void*>(ptr); 
stream_attribute.accessPolicyWindow.num_bytes = num_bytes;     

stream_attribute.accessPolicyWindow.hitRatio  = 0.6;             
stream_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting;
stream_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming; 

cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow, &stream_attribute);
```

### 3.2.4 Shared memory
- 最经典也是最重要的用shared memory优化gemm，要会默写[默写gemm](../gemm/默写gemm.md)

### 3.2.5 Distributed Shared Memory (CC 9.0)
- 一个TBC内部的threads可以互相访问各自的shared memory
- 要保证访问的时候对应的block还在运行，没有退出

```C++
namespace cg = cooperativa_groups;
cg::cluster_group cluster = cg::this_cluster();
__shared__ int smem[];

unsigned int clusterBlockRank = cluster.block_rank();
dst_block_rank = ; // 想访问的shared memory所在block的cluster block rank
cluster.map_shared_rank(smem, dst_block_rank);
```

- 如何指定cluster size详见原文

### 3.2.6 Page-Locked Host Memory
- Page-locked (pinned) host memory有如下好处
	- pinned <-> device memory的copy可以并行执行
	- 带宽高，利用write-combining memory带宽更高（*这个确实，我记得A10上是非pinned的10GB/s，pinned有24GB/s*）
- 操作
	- cudaHostAlloc分配pinned host memory
	- cudaHostRegister把malloc的内存转化成pinned memory，但有局限性
- 多device时
	- 使用**cudaHostAllocPortable**的flag使得所有device都能高效与pinned host memory交互
- write-combing memory
	- 默认的pinned memory是可能进host端的cache的
	- 进了cache就会因为缓存一致性的监听机制影响性能
	- 如果一段内存只需要写不需要读，可以用**cudaHostAllocWriteCombined**的flag标记
	- 不要atomic操作WC的内存
	- 疑问：如果只涉及CPU<->GPU之间的copy，WC memory合适吗？
- mapped memory
	- 可以吧pinned memory映射到device memory上，使用**cudaHostAllocMapped**的flag
	- 在kernel上用cudaHostGetDevicePointer获取其指针，具体详见原文
	- 好处
		- 隐式的device和host之间的数据传输
		- 无需使用stream而自动让kernel execution与数据传输重合
	- 注意align问题，详见原文

### 3.2.7 Memory synchronization domains (CC 9.0)
- memory fence干涉问题
	- system scope的内存同步具有累积性，意味着它不仅会同步当前线程的内存操作，还会同步其他线程的内存操作。GPU在执行时无法知道哪些内存操作是必须同步的，哪些是偶然可见的，因此它会保守地等待所有可能的内存操作完成。这可能导致不必要的等待，尤其是在涉及到较慢的NVLink或PCIe写操作时。
- hopper新机制
	- 每个内核启动分配一个**域ID（Domain ID）**。
	- 写操作和栅栏操作会标记域ID，栅栏仅对同一域内的写操作生效。
	- 例如，计算内核和通信内核可以分属不同域，避免不必要的同步等待。
- 详见原文

### 3.2.8 Asynchronous Concurrent Execution
- Concurrent Host & Device
	- 对于host来说，以下操作可以是异步的，即提交了之后host不会卡住
		- kernel launch
		- device内部的memcpy
		- 小于64KB的hostToDevice的memcpy
		- 带async后缀的操作
		- memset
	- kernel launch在使用profiler的时候是同步的
	- async操作使用了非pinned的内存的时候也可能是同步的
- stream
	- stream内部的操作是顺序的
	- 默认编译`--default-stream legacy` ，所有host的thread默认使用NULL stream，NULL stream不能跟其他的stream同时执行，从而实现隐式同步
	- 编译`--default-stream per-thread`，每个host thread会被分配一个默认的stream（不是NULL stream）
	- 同步
		- `cudaDeviceSynchronize()`等待之前所有stream的指令结束
		- `cudaStreamSynchronize()`等待某个stream的指令结束
		- `cudaStreamWaitEvent()`等待某个event执行
		- `cudaStreamQuery()`非阻塞，询问stream是否执行完毕
	- `cudaLaunchHostFunc()`可以把host端的函数执行加入stream
	- `cudaStreamCreateWithPriority` 可以给stream设置优先级，GPU调度时会参考

### 3.2.8.6 Programmatic Dependent Launch and Synchronization (CC9.0)
- 解决的问题：同stream中，执行顺序kernel 1 -> kernel 2，这么写是因为kernel 2依赖于kernel 1。但是kernel 2有部分初始操作可以不依赖于kernel1先执行。![](../../../../accessories/Pasted%20image%2020250316223308.png)
- kernel 1中使用`cudaTriggerProgrammaticLaunchCompletion()`来激活kernel 2
- kernel 2中使用`cudaGridDependencySynchronize()`来让之前的指令可以与kernel 1同步执行

### 3.2.8.9 CUDA Graphs
- 解决的问题
	- 对于包含 us级别的小kernel但是kernel数量特别多 的任务，CPU端launch kernel的时间可能成了bottleneck
	- CUDA Graph可以把 很多kernel的执行顺序 做成所谓的graph直接发给GPU端，避免了大量单kernel的launch时间浪费

```C++
#define NSTEP 1000
#define NKERNEL 20

// start CPU wallclock timer
for(int istep=0; istep<NSTEP; istep++){
  for(int ikrnl=0; ikrnl<NKERNEL; ikrnl++){
    shortKernel<<<blocks, threads, 0, stream>>>(out_d, in_d);
    cudaStreamSynchronize(stream);
  }
}
//end CPU wallclock time
```

![](../../../../accessories/Pasted%20image%2020250227174650.png)


- 可以看到这个简单的小kernel连续执行（没有memcpy）中间会有较大的CPU CUDA API引起的空隙（虽然时间上很小，但是GPU上执行的更快，CPU端反而成了bottleneck）
- 这个空隙内，CPU端launch了kernel，开启了同步
- kernel在GPU端端执行时间是2.9us，但端到端的kernel平均执行时间却有9.6us

![](../../../../accessories/Pasted%20image%2020250227175443.png)

- 把`cudaStreamSynchronize`移出最内层循环后，情况改善了一些，CPU端可以在上一个kernel未结束时就可以launch下一个kernel，从而hide了CPU launch kernel的时间
- 但是kernel之间还是有少许空隙，平均kernel时间3.8us

```C++
bool graphCreated=false;
cudaGraph_t graph;
cudaGraphExec_t instance;
for(int istep=0; istep<NSTEP; istep++){
  if(!graphCreated){
    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);
    for(int ikrnl=0; ikrnl<NKERNEL; ikrnl++){
      shortKernel<<<blocks, threads, 0, stream>>>(out_d, in_d);
    }
    cudaStreamEndCapture(stream, &graph);
    cudaGraphInstantiate(&instance, graph, NULL, NULL, 0);
    graphCreated=true;
  }
  cudaGraphLaunch(instance, stream);
  cudaStreamSynchronize(stream);
}
```

	- CUDA Graph可以提前计算好kernel在stream中的执行顺序，一次提交，把launch kernel的任务全都扔给了GPU
	- 创建一次Graph的时间可能有点长，但是你可以复用Graph，减少了launch重复多kernel任务的时间浪费
	- 详见原文

### 3.2.8.10 Events
- 用于stream中的同步和时间记录
```C++
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);
cudaEventRecord(start, 0);
for (int i = 0; i < 2; ++i) {
    cudaMemcpyAsync(inputDev + i * size, inputHost + i * size,
                    size, cudaMemcpyHostToDevice, stream[i]);
    MyKernel<<<100, 512, 0, stream[i]>>>
               (outputDev + i * size, inputDev + i * size, size);
    cudaMemcpyAsync(outputHost + i * size, outputDev + i * size,
                    size, cudaMemcpyDeviceToHost, stream[i]);
}
cudaEventRecord(stop, 0);
cudaEventSynchronize(stop);
float elapsedTime;
cudaEventElapsedTime(&elapsedTime, start, stop);
```

### 3.2.9 Multi-Device System
// TODO

### 3.2.10 Unified Virtual Address Space
// TODO

### 3.2.11 Interprocess Communication
// TODO

### 3.2.12  Error Checking
- CUDA的错误码是host thread全局的
	- 通过`cudaPeekAtLastError`获取当前错误码
	- 通过`cudaGetLastError`获取当前错误码并重置为无错误
- 异步操作的错误
	- 异步操作的运行时错误是无法通过其返回值发现的，因为它在运行时才会出错，到时候错误代码只会在后面某个不相关的cuda函数返回时被发现
	- 确定异步操作的错误只能通过在其后立即执行`cudaDeviceSynchronize()`检查返回错误码
- 每个cuda之后立即执行`cudaPeekAtLastError`以检查是否有非运行时错误（比如参数错误等）


